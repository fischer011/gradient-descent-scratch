# -*- coding: utf-8 -*-
"""gradient_descent_scratch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NzBLr6MXjPSMNIb_T7KSWEcIXq4Uxjm4

Testing and observing read in variations.
"""

import numpy as np
import pandas as pd

data = pd.read_csv("/content/clean_weather.csv")
data = data.ffill() # forward fill missing data

data.head(5)

data.describe()

data.columns

data_M = data.to_numpy()

data_M.shape

data2 = pd.read_csv("/content/clean_weather.csv", index_col=0)
data2 = data2.ffill() # forward fill missing data

data2.head(5)

data2.index

data2.columns

data_M2 = data2.to_numpy()

data_M2.shape

"""I didn't initially assign the index_col so row numbers were used and this lead to an additional data column."""

data2.plot.scatter("tmax", "tmax_tomorrow")
data2.plot.scatter("tmin", "tmax_tomorrow")
data2.plot.scatter("rain", "tmax_tomorrow")

import matplotlib.pyplot as plt

data2.plot.scatter("tmax", "tmax_tomorrow")
plt.plot([0, 120], [0, 120], 'red')

"""Linear Regression Equation

$\hat{y} = w_{1}x_{1} + w_{2}x_{2}  + {...} w_{n}x_{n}+ b$
___
Simple Linear Regression Equation (essentially same as equation of a line)

$\hat{y} = w_{1}x_{1} + b$

Example using built-in linear regression model from scikit-learn

https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html
"""

from sklearn.linear_model import LinearRegression

lr = LinearRegression() # .fit(data2[["tmax"]], data2["tmax_tomorrow"])
lr_fitted = lr.fit(data2[["tmax"]], data2["tmax_tomorrow"])
#type(lr)

data2.plot.scatter("tmax", "tmax_tomorrow")
plt.plot([35, 125], [35, 125], "red")
plt.plot(data2["tmax"], lr.predict(data2[["tmax"]]), "green")

lr_fitted.coef_

lr_fitted.intercept_

score = lr_fitted.score(data2[["tmax"]], data2[["tmax_tomorrow"]])
print(score)

print(f"Weight(w):  {lr_fitted.coef_[0]:.6f}")
print(f"Bias(b):  {lr_fitted.intercept_:.6f}")
print(f"Score(R-squared):  {score:.6f}")

"""Measuring loss with mean squared error (MSE)

${\displaystyle \operatorname {MSE} = {{\frac {1}{n}}\sum_{i=1}^{n}\left({\hat{y}} - y\right)^{2}}}$

___
Simple MSE formula

$MSE = (\hat{y} - y)^2$

Example with

let x(tmax)= 80

and

y(tmax_tomorrow actual)= 81
"""

print(lr_fitted.intercept_)

loss = lambda w, y: ((w * 80 + lr_fitted.intercept_) - y) ** 2
y = 81

weights = np.arange(-2, 3, 0.1)
losses = loss(weights, y)

plt.scatter(weights, losses)
# Plotting with calculated weight for least loss.
p = loss(lr_fitted.coef_[0], y)
print("Loss at w = least loss weight: " + str(p))
plt.plot(lr_fitted.coef_[0], loss(lr_fitted.coef_[0], y), 'ro')

"""Gradient of loss on graph is the derivative of the loss.

$f = (\hat{y} - y)^2$

$f' = 2(\hat{y} - y)$
"""

gradient = lambda w, y: 2 * ((w * 80 + lr_fitted.intercept_) - y)

weights = np.arange(-2, 3, 0.1)
gradients = gradient(weights, y)

plt.scatter(weights, gradients)
# Plotting with calculated weight for least loss.
pg0 = gradient(lr_fitted.coef_[0], y)
print("Gradient at w = least loss weight: " + str(pg0))
plt.plot(lr_fitted.coef_[0], gradient(lr_fitted.coef_[0], y), 'ro')

"""I already know the weight with least loss from sklearn built-in linear regression model.  Looking at findings with a starting weight of 2 now."""

weights = np.arange(-2, 3, 0.1)
losses = loss(weights, y)

plt.scatter(weights, losses)
# Plotting with new manual start weight of 2.
p = loss(2, y)
print("Loss at w = 2: " + str(p))
plt.plot(2, loss(2, y), 'ro')

weights = np.arange(-2, 3, 0.1)
gradients = gradient(weights, y)

plt.scatter(weights, gradients)
# Plotting with new manual start weight of 2.
pg = gradient(2, y)
print("Gradient at w = 2: " + str(pg))
plt.plot(2, gradient(2, y), 'ro')

"""Gradients and Partial Derivatives Review

Partial derivative of loss with respect to bias b.

$\frac{\partial L}{\partial b}=\partial L$
"""

partial_d_wrt_b = 2 * ((lr_fitted.coef_[0] * 80 + lr_fitted.intercept_) - y)
print(partial_d_wrt_b)

"""---
Partial derivative of loss with respect to weight w. Using CHAIN RULE:

$$\frac{\partial f}{\partial x}=\frac{\partial f}{\partial y} * \frac{\partial y}{\partial x}$$

$\frac{\partial L}{\partial w}=\frac{\partial L}{\partial (x*w)} * \frac{\partial (x*w)}{\partial w}$

$\space=\frac{\partial L}{1} * \frac{x*1}{1}$

$\space=gradient * {x}$



"""

partial_d_wrt_w = pg0 * 80
print(partial_d_wrt_w)

"""For new weight subract partial derivative with respect to w from prior weight:

For my example this is

w = 2, and

$\frac{\partial L}{\partial w}= gradient * x$

new_weight = 2 - gradient(2, y) * 80
"""

weights = np.arange(-15000, 0, 1000)
losses = loss(weights, y)

plt.scatter(weights, losses)

plt.plot(2, loss(2, y), 'ro')
new_weight = 2 - gradient(2, y) * 80
plt.plot(new_weight, loss(new_weight, y), 'go')

gradients = gradient(weights, y)

plt.scatter(weights, gradients)
plt.plot(2, gradient(2, y), 'ro')

new_wt_grad = gradient(new_weight, y)
print(new_wt_grad)
plt.plot(new_weight, gradient(new_weight, y), 'go')

"""Learning rate reduces size of parameter update so you won't keep taking too big of a step."""

weights = np.arange(0, 2, 0.05)
losses = loss(weights, y)

plt.scatter(weights, losses)

plt.plot(2, loss(2, y), 'ro')
lr = 1e-5
new_weight = 2 - lr * gradient(2, y) * 80
print(new_weight)
plt.plot(new_weight, loss(new_weight, y), 'yo')

"""Another iteration."""

weights = np.arange(0, 2, 0.05)
losses = loss(weights, y)

plt.scatter(weights, losses)

plt.plot(new_weight, loss(new_weight, y), 'ro')
lr = 1e-5
new_weight = new_weight - lr * gradient(new_weight, y) * 80
print(new_weight)
plt.plot(new_weight, loss(new_weight, y), 'yo')

weights = np.arange(0, 2, 0.05)
losses = loss(weights, y)

plt.scatter(weights, losses)
plt.plot(new_weight, loss(new_weight, y), 'ro')
lr = 5e-6
new_weight = new_weight - lr * gradient(new_weight, y) * 80
print(new_weight)

for weight in weights:
  weights = np.arange(0, 2, 0.05)
  losses = loss(weights, y)
  print(weight, new_weight)
  plt.scatter(weights, losses)
  plt.plot(new_weight, loss(new_weight, y), 'ro')

  lr = 5e-5
  new_weight = new_weight - lr * gradient(new_weight, y) * 80
  #print(new_weight)
  plt.plot(new_weight, loss(new_weight, y), 'yo')
  plt.show()