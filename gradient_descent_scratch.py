# -*- coding: utf-8 -*-
"""gradient_descent_scratch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NzBLr6MXjPSMNIb_T7KSWEcIXq4Uxjm4

Testing and observing read in variations.
"""

import numpy as np
import pandas as pd

data = pd.read_csv("/content/clean_weather.csv")
data = data.ffill() # forward fill missing data

data.head(5)

data.describe()

data.columns

data_M = data.to_numpy()

data_M.shape

data2 = pd.read_csv("/content/clean_weather.csv", index_col=0)
data2 = data2.ffill() # forward fill missing data

data2.head(5)

data2.index

data2.columns

data_M2 = data2.to_numpy()

data_M2.shape

"""I didn't initially assign the index_col so row numbers were used and this lead to an additional data column."""

data2.plot.scatter("tmax", "tmax_tomorrow")
data2.plot.scatter("tmin", "tmax_tomorrow")
data2.plot.scatter("rain", "tmax_tomorrow")

import matplotlib.pyplot as plt

data2.plot.scatter("tmax", "tmax_tomorrow")
plt.plot([0, 120], [0, 120], 'red')

"""Linear Regression Equation

$\hat{y} = w_{1}x_{1} + w_{2}x_{2}  + {...} w_{n}x_{n}+ b$
___
Simple Linear Regression Equation (essentially same as equation of a line)

$\hat{y} = w_{1}x_{1} + b$

Example using built-in linear regression model from scikit-learn

https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html
"""

from sklearn.linear_model import LinearRegression

lr = LinearRegression() # .fit(data2[["tmax"]], data2["tmax_tomorrow"])
lr_fitted = lr.fit(data2[["tmax"]], data2["tmax_tomorrow"])

data2.plot.scatter("tmax", "tmax_tomorrow")
plt.plot([35, 125], [35, 125], "red")
plt.plot(data2["tmax"], lr.predict(data2[["tmax"]]), "yellow")
plt.plot(data2["tmax"], lr_fitted.predict(data2[["tmax"]]), "green")

"""Don't see yellow line graphed. Checking if it is the same line as green line with allclose.

Returns True if two arrays are element-wise equal within a tolerance.

https://numpy.org/doc/2.3/reference/generated/numpy.allclose.html#numpy-allclose
"""

np.allclose(lr.predict(data2[["tmax"]]), lr_fitted.predict(data2[["tmax"]]))

lr_fitted.coef_

lr_fitted.intercept_

score = lr_fitted.score(data2[["tmax"]], data2[["tmax_tomorrow"]])
print(score)

print(f"Weight(w):  {lr_fitted.coef_[0]:.6f}")
print(f"Bias(b):  {lr_fitted.intercept_:.6f}")
print(f"Score(R-squared):  {score:.6f}")

"""Measuring loss with mean squared error (MSE)

${\displaystyle \operatorname {MSE} = {{\frac {1}{n}}\sum_{i=1}^{n}\left({\hat{y}} - y\right)^{2}}}$

___
Simple MSE formula

$MSE = (\hat{y} - y)^2$

Example with

let x(tmax)= 80

and

y(tmax_tomorrow actual)= 81
"""

print(lr_fitted.intercept_)

"""Changed verbiage of calculated weight again.  I erroneously assumed the built-in model would find the point at which loss is the least."""

loss = lambda w, y: ((w * 80 + lr_fitted.intercept_) - y) ** 2
y = 81

weights = np.arange(-2, 3, 0.1)
losses = loss(weights, y)

plt.scatter(weights, losses)
# Plotting with calculated weight from built-in model.
p = loss(lr_fitted.coef_[0], y)
print("Loss at w = calculated weight from built-in model: " + str(p))
plt.plot(lr_fitted.coef_[0], loss(lr_fitted.coef_[0], y), 'ro')

"""Gradient of loss on graph is the derivative of the loss.

$f = (\hat{y} - y)^2$

$f' = 2(\hat{y} - y)$
"""

gradient = lambda w, y: 2 * ((w * 80 + lr_fitted.intercept_) - y)

weights = np.arange(-2, 3, 0.1)
gradients = gradient(weights, y)

plt.scatter(weights, gradients)
# Plotting with calculated weight from built-in model.
pg0 = gradient(lr_fitted.coef_[0], y)
print("Gradient at w = calculated weight from built-in model: " + str(pg0))
plt.plot(lr_fitted.coef_[0], gradient(lr_fitted.coef_[0], y), 'ro')

"""First looked at findings with calculated weight from sklearn built-in linear regression model.  Looking at findings with a starting weight of 2 now."""

weights = np.arange(-2, 3, 0.1)
losses = loss(weights, y)

plt.scatter(weights, losses)
# Plotting with new manual start weight of 2.
p = loss(2, y)
print("Loss at w = 2: " + str(p))
plt.plot(2, loss(2, y), 'ro')

weights = np.arange(-2, 3, 0.1)
gradients = gradient(weights, y)

plt.scatter(weights, gradients)
# Plotting with new manual start weight of 2.
pg = gradient(2, y)
print("Gradient at w = 2: " + str(pg))
plt.plot(2, gradient(2, y), 'ro')

"""Gradients and Partial Derivatives Review

Partial derivative of loss with respect to bias b.

$\frac{\partial L}{\partial b}=\partial L$
"""

partial_d_wrt_b = 2 * ((lr_fitted.coef_[0] * 80 + lr_fitted.intercept_) - y)
print(partial_d_wrt_b)

"""---
Partial derivative of loss with respect to weight w. Using CHAIN RULE:

$$\frac{\partial f}{\partial x}=\frac{\partial f}{\partial y} * \frac{\partial y}{\partial x}$$

$\frac{\partial L}{\partial w}=\frac{\partial L}{\partial (x*w)} * \frac{\partial (x*w)}{\partial w}$

$\space=\frac{\partial L}{1} * \frac{x*1}{1}$

$\space=gradient * {x}$



"""

partial_d_wrt_w = pg0 * 80
print(partial_d_wrt_w)

"""For new weight subract partial derivative with respect to w from prior weight:

For my example this is

w = 2, and

$\frac{\partial L}{\partial w}= gradient * x$

new_weight = 2 - gradient(2, y) * 80
"""

weights = np.arange(-15000, 0, 1000)
losses = loss(weights, y)

plt.scatter(weights, losses)

plt.plot(2, loss(2, y), 'ro')
new_weight = 2 - gradient(2, y) * 80
print('new_weight = ' + str(new_weight))
print('Loss = ' +str(loss(new_weight, y)))
plt.plot(new_weight, loss(new_weight, y), 'yo')

gradients = gradient(weights, y)

plt.scatter(weights, gradients)
plt.plot(2, gradient(2, y), 'ro')

new_wt_grad = gradient(new_weight, y)
print('new_wt_grad = ' +str(new_wt_grad))
plt.plot(new_weight, gradient(new_weight, y), 'yo')

"""Learning rate reduces size of parameter update so you won't keep taking too big of a step."""

weights = np.arange(0, 2, 0.05)
losses = loss(weights, y)

plt.scatter(weights, losses)

plt.plot(2, loss(2, y), 'ro')
lr = 1e-5
new_weight = 2 - lr * gradient(2, y) * 80
print('new_weight = ' + str(new_weight))
print('Loss = ' +str(loss(new_weight, y)))
plt.plot(new_weight, loss(new_weight, y), 'yo')

"""Another iteration."""

weights = np.arange(0, 2, 0.05)
losses = loss(weights, y)

plt.scatter(weights, losses)

plt.plot(new_weight, loss(new_weight, y), 'ro')
lr = 1e-5
new_weight = new_weight - lr * gradient(new_weight, y) * 80
print('new_weight = ' + str(new_weight))
print('Loss = ' +str(loss(new_weight, y)))
plt.plot(new_weight, loss(new_weight, y), 'yo')

"""Implementing a for loop to complete the rest of the iterations."""

weights = np.arange(0, 2, 0.05)
losses = loss(weights, y)

plt.scatter(weights, losses)
plt.plot(new_weight, loss(new_weight, y), 'ro')
lr = 5e-6
new_weight = new_weight - lr * gradient(new_weight, y) * 80

for weight in weights:
  weights = np.arange(0, 2, 0.05)
  losses = loss(weights, y)
  plt.scatter(weights, losses)
  plt.plot(new_weight, loss(new_weight, y), 'ro')
  lr = 5e-5
  new_weight = new_weight - lr * gradient(new_weight, y) * 80
  print('new_weight = ' + str(new_weight))
  print('Loss = ' +str(loss(new_weight, y)))
  plt.plot(new_weight, loss(new_weight, y), 'yo')
  plt.show()

"""Linear Regression using all data. Still might want to run all rows with only tmax as predictor.

First defining the predictors and target.
"""

predictors = ["tmax", "tmin", "rain"]
target = "tmax_tomorrow"

np.random.seed(11)

"""Splitting into training, validation, and test sets at 1st section 0-70% of rows, 2nd section 70-85% of rows, and last section 85-100% of rows.

https://numpy.org/doc/stable/reference/generated/numpy.split.html#numpy-split

numpy.split(ary, indices_or_sections, axis=0)


"""

split_data = np.split(data2, [int(0.7 * len(data2)), int(0.85 * len(data2))], axis=0)
split_data[0]

"""Probably getting into the weeds here, but some thoughts:

I don't think that this is the best way to split THIS data.  Is it appropriate to have a weather test set use data from year 2017 forward when the training set starts in year 1970.  Not a ML linear regression question so much as domain expertise issue?

"""

(train_x, train_y), (validate_x, validate_y), (test_x, test_y) = [
                    [d[predictors].to_numpy(), d[[target]].to_numpy()]
                     for d in split_data
                     ]

(test_x, test_y)

import math

def init_params(num_predictors):
  np.random.seed(11)
  weights = np.random.rand(num_predictors, 1)
  biases = np.ones((1,1))
  return [weights, biases]

init_params(3)

def forward(params, x):
  weights, biases = params
  prediction = x @ weights + biases
  return prediction

"""Function to measure loss and gradient.  Using mean squared error."""

def mse(actual, predicted):
  return np.mean((predicted - actual) ** 2)

def mse_grad(actual, predicted):
  return 2 * (predicted - actual)